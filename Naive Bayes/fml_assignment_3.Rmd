---
title: "fml_assignment_3_811289717"
author: "SAKSHI"
date: "2023-10-16"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```

## R Markdown

```{r}
library(caret)
library(e1071)
library(tidyverse)
```

```{r}
accidents = read.csv("/Users/sakshibansal/Downloads/accidentsFull.csv")
```

# Summary
*1. We predicted that injury = yes since the probability of injury happening (0.5087832) is greater than the probability of injury not happening (0.4912168).*

*2.1. Following are the Bayes probability of an injury = yes given all possible combination of weather and traffic parameters- 0.67 , 0.18 , 0 , 0 , 0 , 1.*

*2.2. With 0.5 as cutoff, the 24 records of accidents were classified by model as 10 "YES" and 14 "NO".*

*2.3. The naive Bayes conditional probability of an injury given WEATHER_R = 1 and TRAF_CON_R = 1 is 0.*

*2.4. Yes, the resulting classifications and ranking of the observations is equivalent.*

*3.2 Overall error of validation set is 0.4794951*

### Conclusions dervived from this assignment

*Naive bayes theorem assume that all the variables are independent which is not the case with bayes theorem resulting in different answers.*

*Naive bayes ranking is identical to bayes when we have sufficient data and same class of variables.*

*Naive bayes use "Laplace Smoothing" which assigns random non-zero values to zero-value and one-value probabilities.*

***

# Questions

The file accidentsFull.csv contains information on 42,183 actual automobile accidents in 2001 in the United States that involved one of three levels of injury: NO INJURY, INJURY, or FATALITY. For each accident, additional information is recorded, such as day of week, weather conditions, and road type. A firm might be interested in developing a system for quickly classifying the severity of an accident based on initial reports and associated data in the system (some of which rely on GPS-assisted reporting).

Our goal here is to predict whether an accident just reported will involve an injury (MAX_SEV_IR = 1 or 2) or will not (MAX_SEV_IR = 0). For this purpose, create a dummy variable called INJURY that takes the value “yes” if MAX_SEV_IR = 1 or 2, and otherwise “no.”

**1.Using the information in this dataset, if an accident has just been reported and no further information is available, what should the prediction be? (INJURY = Yes or No?) Why?**

**Answer 1**

```{r}

# Creating a dummy variable called injury

accidents$injury = ifelse(accidents$MAX_SEV_IR > 0, "yes" , "no")

# Finding the count for "yes" and "no"

t=table(accidents$injury)
t

# Finding the probability of injury not happening

injuryno = t["no"]/nrow(accidents)
injuryno

# Finding the probability of injury happening

injuryyes = t["yes"]/nrow(accidents)
injuryyes
```

As we can see that probability of injury happening (0.5087832) is greater than the probability of injury not happening (0.4912168), we can safely assume that if an accident had just been reported and no further information is available , then we can predict that there has been an injury.

***

**2.Select the first 24 records in the dataset and look only at the response (INJURY) and the two predictors WEATHER_R and TRAF_CON_R. Create a pivot table that examines INJURY as a function of the two predictors for these 24 records. Use all three variables in the pivot table as rows/columns.**

```{r}

#Selecting first 24 records in the dataset

df = accidents[1:24,]

#Making a pivot table of all three variables

df = df %>%
  select(injury,WEATHER_R,TRAF_CON_R)

prob.df = ftable(df)
prob.df

prob.df.2 = ftable(df[,-1])
prob.df.2
```

**2.1. Compute the exact Bayes conditional probabilities of an injury (INJURY = Yes) given the six possible combinations of the predictors.**

Below is the probability of an injury = YES when we are considering six possible combinations of the predictors: 

*TRAF_CON_R = 0, WEATHER_R = 1*

```{r}
prob.yes.1 = round(prob.df[3,1]/prob.df.2[1,1],2)
prob.yes.1
```

*TRAF_CON_R = 0, WEATHER_R = 2*

```{r}
prob.yes.2 = round(prob.df[4,1]/prob.df.2[2,1],2)
prob.yes.2
```

*TRAF_CON_R = 1, WEATHER_R = 1*

```{r}
prob.yes.3 = prob.df[3,2]/prob.df.2[1,2]
prob.yes.3
```

*TRAF_CON_R = 1, WEATHER_R = 2*

```{r}
prob.yes.4 = prob.df[4,2]/prob.df.2[2,2]
prob.yes.4
```

*TRAF_CON_R = 2, WEATHER_R = 1*

```{r}
prob.yes.5 = prob.df[3,3]/prob.df.2[1,3]
prob.yes.5
```

*TRAF_CON_R = 2, WEATHER_R = 2*

```{r}
prob.yes.6 = prob.df[4,3]/prob.df.2[2,3]
prob.yes.6
```

**2.2 Classify the 24 accidents using these probabilities and a cutoff of 0.5.**

```{r}
prob.accidents = rep(0,24)

# Creating a new variable and putting loop to determine the values of all probabilities given that below conditions are met.

prob.injury = prob.accidents
for (i in 1:24) {
      if (df$WEATHER_R[i] == "1") {
      if (df$TRAF_CON_R[i]=="0"){
        prob.injury[i] = prob.yes.1
      }
      else if (df$TRAF_CON_R[i]=="1") {
        prob.injury[i] = prob.yes.3
      }
      else if (df$TRAF_CON_R[i]=="2") {
        prob.injury[i] = prob.yes.5
      }
    }
    else {
      if (df$TRAF_CON_R[i]=="0"){
        prob.injury[i] = prob.yes.2
      }
      else if (df$TRAF_CON_R[i]=="1") {
        prob.injury[i] = prob.yes.4
      }
      else if (df$TRAF_CON_R[i]=="2") {
        prob.injury[i] = prob.yes.6
      }
    }
}
df$probablity = prob.injury

# Predicting the possibility of accidents

df$prediction = ifelse(df$probablity > 0.5,"yes","no")

head(df , 24)
```

**2.3 Compute manually the naive Bayes conditional probability of an injury given WEATHER_R = 1 and TRAF_CON_R = 1.**

```{r}

# Applying naive bayes formula assuming that weather and injury are independent variables.

prob.injury.2 = ((sum(prob.df[3,])/sum(prob.df[c(3,4),]))*(sum(prob.df[c(3,4),2])/sum(prob.df[c(3,4),])))/(sum(prob.df[c(1,3),])/sum(prob.df))*(sum(prob.df[,2])/sum(prob.df))

prob.injury.2
```

**2.4 Run a naive Bayes classifier on the 24 records and two predictors. Check the model output to obtain probabilities and classifications for all 24 records. Compare this to the exact Bayes classification. Are the resulting classifications equivalent? Is the ranking (= ordering) of observations equivalent?**

```{r}

# Converting every variable into factors using as.factor

for (i in c(1:dim(accidents)[2])){
  accidents[,i] <- as.factor(accidents[,i])
}

# Adding laplace = 0 in order to to stop R to assign random non-zero values to probability = 0,1 and using naive bayes function

prob.naive <- naiveBayes(injury ~ TRAF_CON_R + WEATHER_R, 
                 data = df,laplace = 0)

prob.naive.2 <- predict(prob.naive, newdata = df,type = "raw")

prob.naive.2 = round(prob.naive.2,2)

df$prob.naive.3 <- prob.naive.2[,2]

df$prob.naive.4 = ifelse(df$prob.naive.3 > 0.5,"yes","no")

head(df,10) %>% 
  select(prediction,prob.naive.4)
```

The laplace was put as 0 because the naive bayes theorem uses laplace smoothing which randomly assigns non-zero values to records with probability of 0,1. In any other case, it would have been an advantage , but here we had to find out the ranking order between naive and bayes theorem, so we had to remove all the zero and one value probabilities since bayes theorem does not follow laplace smoothing and we would have ended up with inequal order. 

```{r}

# Arranging by bayes theorem

bayes.rank = df %>% 
  select(probablity,prob.naive.3) %>% 
  filter(!probablity==0) %>% 
  filter(!probablity==1) %>%
  arrange(probablity)
bayes.rank = rank(bayes.rank)

# Arranging by naive bayes theorem 

naive.rank = df %>% 
  select(probablity,prob.naive.3) %>% 
  filter(!probablity==0) %>%
  filter(!probablity==1) %>%
  arrange(prob.naive.3)
naive.rank = rank(naive.rank)

#Comparison of both ranks

all(bayes.rank == naive.rank)
```

***

**3. Let us now return to the entire dataset. Partition the data into training (60%) and validation (40%).**

```{r}

# Partitioning the data into training set (60%) and validation set (40%).

accident.train = sample(row.names(accidents),0.6*dim(accidents)[1])

accident.valid = setdiff(row.names(accidents),accident.train)

train.df = accidents[accident.train,-24]

valid.df = accidents[accident.valid,-24]
```

**3.1 Run a naive Bayes classifier on the complete training set with the relevant predictors (and INJURY as the response). Note that all predictors are categorical. Show the confusion matrix.**

```{r}

#Using naive bayes theorem

naive.prob <- naiveBayes(injury ~ TRAF_CON_R + WEATHER_R, 
                 data = train.df)

naive.prob.2 <- predict(naive.prob , newdata = train.df , type = "raw")

naive.prob.2.pred = ifelse(naive.prob.2[,2] >0.5 ,"yes","no")

naive.prob.2.pred = as.factor(naive.prob.2.pred)

df.matrix = confusionMatrix(train.df$injury,naive.prob.2.pred,positive = "yes")

df.matrix
```

**3.2 What is the overall error of the validation set?**

```{r}

naive.prob.3 <- naiveBayes(injury ~ TRAF_CON_R + WEATHER_R, 
                 data = valid.df)

naive.prob.4 <- predict(naive.prob.3,newdata = valid.df,type = "raw")

naive.prob.4.pred = ifelse(naive.prob.4[,2] >0.5 ,"yes","no")

naive.prob.4.pred = as.factor(naive.prob.4.pred)

df.matrix.2 = confusionMatrix(valid.df$injury,naive.prob.4.pred,positive = "yes")
```

Error rate = 1-accuracy

```{r}
Error = 1- df.matrix.2$overall[1]
Error
```

***






